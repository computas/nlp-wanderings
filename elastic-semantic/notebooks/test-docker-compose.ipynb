{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# To autoreload the module\n",
    "%load_ext autoreload\n",
    "\n",
    "import sys  \n",
    "sys.path.insert(0, '../pipeline')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test tika"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "# import extract_doki_data\n",
    "# this didn't work, seems there is some package issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_with_tika(file_path, tika_server_url='http://localhost:9998/tika'):\n",
    "    \"\"\"\n",
    "    Send a file for PDF extraction to the tika server\n",
    "    Inputs:\n",
    "     - file_path: path to PDF file. It has to be validated beforehand at this is a PDF file.\n",
    "     - tika_server_url: URL to your tika server\n",
    "\n",
    "    Outputs:\n",
    "     - The text returned from the service or a \"blank\" string\n",
    "    \"\"\"\n",
    "    headers = {\n",
    "        'Content-type': 'application/pdf',\n",
    "    }\n",
    "    response = None\n",
    "    try:    \n",
    "        with open(file_path, 'rb') as file:\n",
    "            data = file.read()\n",
    "            response = requests.put(tika_server_url, headers=headers, data=data)\n",
    "            response.raise_for_status()\n",
    "    except requests.exceptions.HTTPError as http_error:\n",
    "        print (\"Http Error:\")\n",
    "    except requests.exceptions.ConnectionError as connection_error:\n",
    "        print (\"Error Connecting:\")\n",
    "    except requests.exceptions.Timeout as timeout_error:\n",
    "        print (\"Timeout Error:\")\n",
    "    \n",
    "    text = ''\n",
    "    if response.status_code == 200:\n",
    "        text = response.content.decode(\"utf-8\") \n",
    "        \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_text_with_tika(file_path='norwegian-nli.pdf',tika_server_url='http://tika:9998/tika')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Elasticsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "\n",
    "es=Elasticsearch([{'host':'elasticsearch-ltr','port':9200}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'epoch': '1581983237',\n",
       "  'timestamp': '23:47:17',\n",
       "  'cluster': 'docker-cluster',\n",
       "  'status': 'green',\n",
       "  'node.total': '1',\n",
       "  'node.data': '1',\n",
       "  'shards': '3',\n",
       "  'pri': '3',\n",
       "  'relo': '0',\n",
       "  'init': '0',\n",
       "  'unassign': '0',\n",
       "  'pending_tasks': '0',\n",
       "  'max_task_wait_time': '-',\n",
       "  'active_shards_percent': '100.0%'}]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "es.cat.health(format='json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'health': 'green',\n",
       "  'status': 'open',\n",
       "  'index': '.kibana_task_manager_1',\n",
       "  'uuid': '4tS5Z16RQtuDQmFGQPQUxw',\n",
       "  'pri': '1',\n",
       "  'rep': '0',\n",
       "  'docs.count': '2',\n",
       "  'docs.deleted': '1',\n",
       "  'store.size': '16.3kb',\n",
       "  'pri.store.size': '16.3kb'},\n",
       " {'health': 'green',\n",
       "  'status': 'open',\n",
       "  'index': '.apm-agent-configuration',\n",
       "  'uuid': '8gnK0b3uRP-S0Qg9APPr7g',\n",
       "  'pri': '1',\n",
       "  'rep': '0',\n",
       "  'docs.count': '0',\n",
       "  'docs.deleted': '0',\n",
       "  'store.size': '283b',\n",
       "  'pri.store.size': '283b'},\n",
       " {'health': 'green',\n",
       "  'status': 'open',\n",
       "  'index': '.kibana_1',\n",
       "  'uuid': 'kQ4rEfAfQfWcmcf8kzVmBQ',\n",
       "  'pri': '1',\n",
       "  'rep': '0',\n",
       "  'docs.count': '2',\n",
       "  'docs.deleted': '0',\n",
       "  'store.size': '11.2kb',\n",
       "  'pri.store.size': '11.2kb'}]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "es.cat.indices(format='json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Language models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dense.py  load_models.py  pipeline  requirements.txt  work\n",
      "Loading bert-base-nli-mean-tokens\n",
      "loading distilbert-base-multilingual-cased\n",
      "loading bert-base-multilingual-uncased\n"
     ]
    }
   ],
   "source": [
    "!ls ../\n",
    "!python ../load_models.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
